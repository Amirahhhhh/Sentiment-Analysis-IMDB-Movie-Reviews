# üé¨ Sentiment-Analysis-IMDB-Movie-Reviews
Sentiment analysis on IMDB movie reviews using classical machine learning and deep learning models, with evaluation, tuning, and visualisation techniques.

## 1.	üìå PROJECT OVERVIEW
This project applies machine learning and deep learning techniques to perform sentiment classification on the IMDB movie review dataset, which contains 50,000 labelled movie reviews. The reviews are categorized as either positive or negative, enabling the development of a binary classification model. This task falls under Natural Language Processing (NLP) and text analytics, making it an ideal case study for comparing traditional ML and DL approaches.
The dataset is split into 25,000 reviews for training and 25,000 for testing, offering a robust benchmark for experimentation with various models and techniques.

## 2.	üìä ABOUT THE DATASET
The dataset consist of:

|     Feature names                  |     Description                                        |
|------------------------------------|--------------------------------------------------------|
|     Reviews                        |     Text of movie review                               |
|     Sentiment                      |     Lable: positive or negative                        |

Dataset can be access through the link : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

## 3.	üéØ OBJECTIVE
‚Ä¢  To predict the sentiment (positive or negative) of a given movie review uwing machine learning and deep learning  
‚Ä¢  To compare traditional ML algorithms and advanced DL models like LSTM and CNN. 
‚Ä¢  To understand the impact of hyperparameter tuning and text representation techniques (TF-IDF vs. embeddings).

## 4.	üîé METHODOLOGY
a)	Data Preprocessing
‚Ä¢  Cleaned text: removed HTML tags, punctuation, numbers, and stop words
‚Ä¢  Converted to lowercase and performed lemmatization
‚Ä¢  Balanced classes across training and testing sets

b)	Exploratory Data Analysis (EDA)
‚Ä¢  Word cloud visualization of frequent words in positive vs. negative reviews
‚Ä¢  Class distribution, word count histograms
‚Ä¢  Sentiment polarity trend across the dataset

c)	Data Transformation
‚Ä¢  Tokenization and padding for deep learning models
‚Ä¢  TF-IDF vectorization for traditional ML models
‚Ä¢  Label encoding for binary sentiment output

d)	Model Building & Evaluation
i)	Machine Learning Models
‚Ä¢	Na√Øve Bayes
‚Ä¢	Stochastic Gradient Descent (SGD)
‚Ä¢	XGBoost

ii)	Deep Learning Models
‚Ä¢	LSTM with GloVe embeddings
‚Ä¢	CNN with 1D convolutional layers

iii)	Evaluation Metrics
‚Ä¢	Accuracy, Precision, Recall, F1-Score
‚Ä¢	ROC Curve and AUC Score
‚Ä¢	Plotted Before vs After Hyperparameter Tuning results

e)	Hyperparameter Tuning 
‚Ä¢  Used GridSearchCV for traditional models (SGD, XGBoost)
‚Ä¢  Applied custom Keras Tuner for LSTM & CNN
‚Ä¢  Visualized performance differences pre- and post-tuning in subplots

## 5.	‚öôÔ∏è RESULT 

|        Model          |     Accuracy (Before tuning)     |     Accuracy (After tuning)     |
|-----------------------|----------------------------------|---------------------------------|
|     Naive Bayes       |             0.8634               |             0.8634              |
|     SGD               |             0.8474               |             0.8973              |
|     XGBoost           |             0.8564               |             0.8278              |
|     LSTM              |             0.6920               |             0.8668              |
|     CNN               |             0.8427               |             0.8537              |

‚úÖ SGD and LSTM showed the highest improvement after tuning
‚úÖ Best performing model overall: SGD (Traditional) and LSTM (Deep Learning)

## 6.	üìù CONCLUSION
‚Ä¢  The project demonstrated traditional ML models (SGD) can be highly competitive for text classification. 
‚Ä¢  Deep learning models like LSTM improved significantly after tuning, highlighting the importance of architecture and optimization. 
‚Ä¢  TF-IDF was effective for ML models , while GloVe embeddings enhanced DL models. 





